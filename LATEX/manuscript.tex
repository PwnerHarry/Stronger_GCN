\documentclass{article}
\usepackage[preprint, nonatbib]{neurips_2019}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{epstopdf}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{pxfonts}
\usepackage{amsmath,amssymb,bm}
\usepackage{footnote}
\usepackage{enumerate}
\usepackage{physics}
\usepackage{enumitem}
\usepackage[english]{babel}
\usepackage{comment}
\usepackage{setspace}
\usepackage{algorithm}
\usepackage{algpseudocode}
\newcommand{\INDSTATE}[1][1]{\STATE\hspace{#1\algorithmicindent}}
\usepackage{forloop}
\usepackage{chngpage}
\usepackage{color, colortbl}
\usepackage{booktabs, multirow}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\range}{range}
\DeclareMathOperator{\spn}{span}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\X}{\mathbb{X}}
\newcommand\etal{\textit{et al.}}
\newcommand\ie{\textit{i.e.}}
\newcommand\eg{\textit{e.g.}}
\newcommand\st{\textit{s.t.}}
\newcommand\wrt{\textit{w.r.t.}}
\newcommand\etc{\textit{etc.}}
\newcommand\doubleE{\mathbb{E}}
\newcommand\doubleP{\mathbb{P}}
\newcommand\doubleR{\mathbb{R}}
\newcommand\scriptS{\mathcal{S}}
\newcommand\scriptO{\mathcal{O}}
\newcommand\scriptA{\mathcal{A}}
\newcommand\scriptX{\mathcal{X}}
\newcommand{\colvec}[2][1]{%
	\scalebox{#1}{%
		\renewcommand{\arraystretch}{1.45}%
		$\begin{bmatrix}#2\end{bmatrix}$%
	}
}
\makeatletter
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother

\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
		\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
		\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
		\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{reflection}[2][Reflection]{\begin{trivlist}
		\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
		\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
		\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
		\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proof}{{\noindent\it Proof}\quad}{\hfill $\square$\par}

\def\coef_vec{
	\begin{bmatrix}
		\frac{g^{(0) } (0)}{0!} \\[6pt]
		\frac{g^{(1) } (0)}{1!} \\[6pt]
		\frac{g^{(2) } (0)}{2!}\\[6pt]
		\vdots\\[6pt]
		\frac{g^{(\infty) } (0)}{\infty!}
\end{bmatrix}}

\makeatletter
\newsavebox{\@brx}
\newcommand{\llangle}[1][]{\savebox{\@brx}{\(\m@th{#1\langle}\)}%
	\mathopen{\copy\@brx\kern-0.5\wd\@brx\usebox{\@brx}}}
\newcommand{\rrangle}[1][]{\savebox{\@brx}{\(\m@th{#1\rangle}\)}%
	\mathclose{\copy\@brx\kern-0.5\wd\@brx\usebox{\@brx}}}
\makeatother

\newtheorem{definition}{Definition}

\newcommand{\inprod}[2]{{\llangle #1 ,\;}{#2\rrangle}}

\title{Break the Ceiling: Stronger Multi-scale Deep Graph Convolutional Networks}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

% \author[add1,add2]{Sitao Luan}
% \author[add1,add2]{Mingde Zhao}
% \author[add1]{Xiao-Wen Chang}
% \author[add1,add2,add3]{Doina Precup}

% \address[add1]{McGill University}
% \address[add2]{Mila}
% \address[add3]{DeepMind}


\author{
Sitao Luan$^{1,2}$, Mingde Zhao$^{1,2}$, Xiao-Wen Chang$^{1}$, Doina Precup$^{1,2,3}$\\
\{sitao.luan@mail, mingde.zhao@mail, chang@cs, dprecup@cs\}.mcgill.ca\\
$^1$McGill University; $^2$Mila; $^3$DeepMind
}



% \author{
% \small
% \begin{tabular}[t]{cccc}
% 		Sitao Luan &Mingde Zhao & Xiao-Wen Chang & Doina Precup\\
% 		{sitao.luan@mail.mcgill.ca} & {mingde.zhao@outlook.com} & {chang@cs.mcgill.ca} & {dprecup@cs.mcgill.ca} \\
% 			{McGill University, Mila} & {McGill University, Mila} & {McGill University} & {McGill University, Mila, Deepmind} \\
% \end{tabular}
% }


\begin{document}
\maketitle	

\begin{abstract}
Recently, neural network based approaches have achieved significant improvement for solving large, complex, graph-structured problems. However, their bottlenecks still need to be addressed, and the advantages of multi-scale information and deep architectures have not been sufficiently exploited. In this paper, we theoretically analyze how existing Graph Convolutional Networks (GCNs) have limited expressive power due to the constraint of the activation functions and their architectures. We generalize spectral graph convolution and deep GCN in block Krylov subspace forms and devise two architectures, both with the potential to be scaled deeper but each making use of the multi-scale information in different ways. We further show that the equivalence of these two architectures can be established under certain conditions. On several node classification tasks, with or without the help of validation, the two new architectures achieve better performance compared to many state-of-the-art methods.
%We also show that any well-defined graph convolution operation with an analytic spectral filter can be written as the product of a block Krylov matrix and a learnable parameter matrix with a special form.
\end{abstract}
	
\section{Introduction and Motivation}\label{sec:introduction}
Many real-world problems can be modeled as graphs \cite{hamilton2017inductive, kipf2016classification, liao2019lanczos, gilmer2017neural, monti2017geometric, defferrard2016fast}. Among the recent focus of applying machine learning algorithms on solving these problems, (graph) neural network stands out as one of the most powerful tools. Inspired by the success of convolutional neural networks (CNNs) \cite{lecun1998gradient} in computer vision, researchers have started to transplant the concept of convolution to graphs, which has become one of the key operations in these tasks \cite{li2018adaptive}. In this paper, we focus on spectrum-free Graph Convolutional Networks (GCNs) \cite{bronstein2016geometric, shuman2012emerging}, which have obtained state-of-the-art performance on multiple transductive and inductive learning tasks \cite{defferrard2016fast, kipf2016classification, liao2019lanczos, chen2018fastgcn, chen2017stochastic}.
%There are two main categories of graph convolution: spectral convolution and spatial convolution\cite{zhou2018graph, wu2019survey}. And there are two basic methods to define spectral convolution: spectral methods \cite{bruna2014spectral} and spectrum-free methods\cite{bronstein2016geometric, shuman2012emerging}. Spectrum-free methods become popular these years and have obtained state-of-the-arts performance on multiple transductive learning and inductive learning tasks \cite{defferrard2016fast, kipf2016classification, liao2019lanczos, chen2018fastgcn, chen2017stochastic}.
\par
One big challenge for the existing GCNs is the limited expressive power of their shallow learning mechanisms \cite{zhang2018graph, wu2019survey}. The difficulty of extending GCNs to richer architectures leads to several possible explanations and even some opinions that express the unnecessities of addressing such a problem:
\begin{enumerate}[leftmargin=12pt]
\item Graph convolution can be considered as a special form of Laplacian smoothing \cite{li2018deeper}. A network with multiple convolutional layers will suffer from an over-smoothing problem that makes the representation of the nodes indistinguishable even for the nodes that are far from each other \cite{zhang2018graph}. %{(\bf Do the two sentences have some connections?)}
\item For many cases, it is not necessary for the label information to totally traverse the entire graph. Moreover, one can operate on the multi-scale coarsening of the input graph and obtain the same flow of information as GCNs with more layers \cite{bronstein2016geometric}.
\end{enumerate}
\par
Nevertheless, shallow learning mechanisms violate the compositionality principle of deep learning \cite{lecun2015deep, hinton2006fast} and restrict label propagation \cite{sun2019stage}. In this paper, we first give theoretical analyses of the lack of scalability of the existing GCN. Then we show that any graph convolution with a well-defined analytic spectral filter can be written as a product of a block Krylov matrix and a learnable parameter matrix in a special form. Based on the analyses, we propose two GCN architectures that leverage multi-scale information with different methods and scalability to deeper and richer structures, with the expectation of having stronger expressive powers and abilities to extract richer representations of graph-structured data. We also show that the equivalence of the two architectures can be achieved under certain conditions. For validation, we test the proposed architectures on multiple transductive tasks using their different instantiations. The results show that even the simplest instantiation of the proposed architectures (with no activation functions) yields state-of-the-art performance and the complex ones achieve surprisingly higher performance, both with or without validation.

\section{Preliminaries}
\label{sec:preliminaries}
We use bold font for vectors $\bm{v}$, block vectors $\bm{V}$ and matrix blocks $\bm{V_i}$ as \cite{frommer2017block}. Suppose we have an undirected graph $\mathcal{G}=(\mathcal{V},\mathcal{E}, A)$, where $\mathcal{V}$ is the node set with $\abs{\mathcal{V}}=N$, $\mathcal{E}$ is the edge set with $\abs{\mathcal{E}}=E$, and $A \in \mathbb{R}^{N\times N}$ is a symmetric adjacency matrix. Let $D$ denote the diagonal degree matrix, \ie{} $D_{ii} = \sum_j A_{ij}$. A diffusion process on $\mathcal{G}$ can be defined by a diffusion operator $L$ \cite{coifman2006diffusion, coifman2006diffusionmaps} which is a symmetric positive semi-definite matrix, \eg{} graph Laplacian $L=D-A$, normalized graph Laplacian $L=I-D^{-1/2} A D^{-1/2}$ and affinity matrix $L = A + I$, \etc{} We use $L$ to denote a general diffusion operator in this paper. The eigendecomposition of $L$ gives us $L=U \Lambda U^T$, where $\Lambda$ is a diagonal matrix whose diagonal elements are eigenvalues and the columns of $U$ are the orthonormal eigenvectors and named graph Fourier basis. We also have a feature matrix (graph signals, can be regrarded as a block vector) $\bm{X} \in \mathbb{R}^{N\times F}$ defined on $\mathcal{V}$ and each node $i$ has a feature vector $\bm{X_{i,:}}$, which is the $i^{th}$ row of $X$.

Graph convolution is defined in graph Fourier domain \st{} $\bm{x} *_{\mathcal{G}} \bm{y} = U((U^T \bm{x}) \odot (U^T\bm{y}))$, where $\bm{x}, \bm{y} \in \mathbb{R}^N$ and $\odot$ is the Hadamard product \cite{defferrard2016fast}. Following from this definition, a graph signal $\bm{x}$ filtered by $g_\theta$ can be written as
\begin{equation}\label{def}
    \bm{y} = g_\theta(L)\bm{x} = g_\theta(U \Lambda U^T) \bm{x} = U g_\theta(\Lambda) U^T \bm{x}
\end{equation}
where $g_\theta$ can be any function which is analytic inside a closed contour which encircles $\lambda(L)$, \eg{} Chebyshev polynomial \cite{defferrard2016fast}. GCN generalizes this definition to signals with $F$ input channels and $O$ output channels and the network structure is
\begin{equation}
    \label{eq0}
   \bm{Y} = \text{softmax} ({L} \; \text{ReLU} ( L \bm{X} W_0 ) \; W_1 )
\end{equation}
where $L = D^{-1/2} \tilde{A} D^{-1/2}$ and $\tilde{A} = A+I$. This is called spectrum-free method \cite{bronstein2016geometric} that requires no explicit computation of eigendecomposition \cite{zhang2018graph} and operations on the frequency domain. We will focus on the analysis of GCN in the following sections.

%We do not distinguish vector $v \in \mathbb{R}^{s}$ and block vector $\bm{V} \in \mathbb{R}^{s_1 \times s_2}$ in this section.

%%%So we can write all the features into a matrix $\bm{X}=[\bm{x_1}, \bm{x_2},\dots,\bm{x_F}]\in R^{N\times F}$ and each $\bm{x_i}  (i=1,2,\dots,F)$ is a feature variable. $y\in R^{N\times 1}$ is the label vector and only a small subset of it is available. We have a matrix $L\in R^{N\times N}$ that contains the information from $\mathcal{E}$ and it can represent the relational structures of each pair of nodes. $L$ can be graph Laplacian, affinity matrix of any other kind of similarity matrix or diffusion matrix (operator) that defined on graph. Here, we use graph Laplacian for our analysis.

%%%In GCN, the graph convolution is defined as
%%%%We combine each convolved column by learnable weight matrix $W\in R^{F\times l_1}$ and put a nonlinear activation function on it, then we send the output to the next layer. The input of the next layer is a $N\times l_1$ matrix.
\section{Why GCN is not Scalable?}
Suppose we scale GCN to a deeper architecture in the same way as \cite{kipf2016classification, li2018deeper}, the network becomes
\begin{equation}\label{eq1}
\bm{Y} = \text{softmax} ({L} \; \text{ReLU} ( \cdots L \; \text{ReLU} (L\; \text{ReLU} (L \bm{X} W_0 ) \; W_1 )\; W_2 \cdots ) \; W_n ) \equiv  \text{softmax} (\bm{Y'})
\end{equation}
We have the following theorems.
\begin{theorem} 1 \label{thm1}
Suppose that $\mathcal{G}$ has $k$ connected components and the diffusion operator $L$ is defined as that in \eqref{eq0}. Let $\bm{X}$ be any block vector sampled from space $\mathbb{R}^{N \times F}$ according to a continuous distribution and $\{W_0, W_1, \dots, W_n\}$ be any set of parameter matrices, if $\mathcal{G}$ has no bipartite components, then in \eqref{eq1}, as $n \to \infty$, $\text{rank}(\bm{Y'}) \leq k$ almost surely.
%, consider the space $\mathbb{R}^{N \times F}$ of all block vectors $\bm{X}$ and any set of parameter matrices $\{W_0, W_1, \dots, W_n\}$, we have $\text{rank}(\bm{Y'}) \leq k$ with probability 1, as $n\rightarrow \infty$.
\end{theorem}	
\begin{proof}
See Appendix.
\end{proof}

\begin{theorem} 2
\label{thm2}
Suppose we randomly sample $\bm{x}, \bm{y} \in \mathbb{R}^N$ under a continuous distribution and point-wise function $\text{Tanh}(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$, we have
$$\doubleP(\text{rank}\left(\text{Tanh}([\bm{x},\bm{y}])\right) \geq \text{rank}([\bm{x},\bm{y}]) \;|\; \bm{x},\bm{y} \in \mathbb{R}^N) = 1$$
\end{theorem}

\begin{proof}
See Appendix.
\end{proof}

Theorem 1 shows that if we simply increase the depth based on GCN architecture, the extracted features $\bm{Y'}$ will at most encode stationary information of graph structure and lose all the information in node features. In addition, from the proof we see that the point-wise ReLU transformation is a conspirator. Theorem 2 tells us that Tanh is better in keeping linear independence among column features. We design a numerical experiment on synthetic data (see Appendix) to test, under a 100-layer GCN architecture, how activation functions affect the rank of the output in each hidden layer during the feed-forward process. As Figure 1(a) shows, the rank of hidden features decreases rapidly with ReLU, while having little fluctuation under Tanh, and even the identity function performs better than ReLU (see Appendix for more comparisons). So we propose to replace ReLU by Tanh.%ReLU is also found to have numerical stability issue in the bottom layers of GCN.
\begin{figure*}[htbp]
\centering
\subfloat[Deep GCN]{
\captionsetup{justification = centering}
\includegraphics[width=0.32\textwidth]{{fig_GCN_compare}.pdf}}
\hfill
\subfloat[Snowball]{
\captionsetup{justification = centering}
\includegraphics[width=0.32\textwidth]{{fig_snowball_compare}.pdf}}
\hfill
\subfloat[Truncated Block Krylov]{
\captionsetup{justification = centering}
\includegraphics[width=0.32\textwidth]{{fig_truncated_krylov_compare}.pdf}}
\caption{Number of independent column features }
\label{activation_functions}
\end{figure*}
\section{Spectral Graph Convolution and Block Krylov Subspace Methods}
\subsection{Notation and Backgronds}
%Let $\mathbb{S}$ be a $^*$-subalgebra  with identity $I_s$, \ie{}
Let $\mathbb{S}$ be a block vector subspace of $\mathbb{R}^{F\times F}$ containing the identity matrix $I_F$ that is closed under matrix multiplication and transposition. We define an inner product $\langle\cdot, \cdot\rangle_{\mathbb{S}}$ in the block vector space $\mathbb{R}^{N \times F}$ as follows \cite{frommer2017block},
\begin{definition}
A mapping $\langle\cdot, \cdot\rangle_{\mathbb{S}}$ from $\mathbb{R}^{N\times F} \times \mathbb{R}^{N\times F}$ to $\mathbb{S} $ is called a block inner product onto $\mathbb{S}$ if it satisfies the following conditions for all $\bm{X, Y, Z} \in \mathbb{R}^{N\times F}$ and $C \in \mathbb{S}$:
\begin{enumerate}[leftmargin=12pt]
\item $\mathbb{S}$-linearity: $\langle \bm{X}, \bm{Y}C \rangle_{\mathbb{S}} =  \langle \bm{X}, \bm{Y}\rangle_{\mathbb{S}}C$  and $ \langle \bm{X} + \bm{Y}, \bm{Z} \rangle_{\mathbb{S}} = \langle\bm{X}, \bm{Z}\rangle_{\mathbb{S}} + \langle \bm{Y}, \bm{Z}\rangle_{\mathbb{S}}$
\item symmetry: $ \langle \bm{X}, \bm{Y}\rangle_{\mathbb{S}} = \langle\bm{Y}, \bm{X}\rangle_{\mathbb{S}}^T $
\item definiteness: $ \langle \bm{X}, \bm{X}\rangle_{\mathbb{S}} $ is positive definite if $\bm{X}$ has full rank, and $ \langle \bm{X}, \bm{X}\rangle_{\mathbb{S}} = 0_F$ if and only if $\bm{X} = 0.$
\end{enumerate}
\end{definition}
%\begin{definition}
%A mapping $N$ which maps all $\bm{X} \in \mathbb{R}^{n\times s}$ with full rank to a matrix $N(\bm{X}) \in \mathbb{S}$ is called a scaling quotient if for all such $\bm{X}$ there exists $\bm{Y}\in \mathbb{R}^{n\times s}$ such that $\bm{X}= \bm{Y} N(\bm{X})$ and $ \langle\bm{Y}, \bm{Y}\rangle_{\mathbb{S}} = I_s$ .
%\end{definition}
	
%Two block vectors $\bm{X}, \bm{Y}$ are called $\langle\cdot, \cdot\rangle_{\mathbb{S}} $-orthogonal if $\langle \bm{X}, \bm{Y}\rangle_{\mathbb{S}} = 0_s$ and we call a block vector $\langle\cdot, \cdot\rangle_{\mathbb{S}} $-normalized if $ \langle \bm{X}, \bm{X}\rangle_{\mathbb{S}} = I_s$
There are mainly three ways to define $\langle\cdot, \cdot\rangle_{\mathbb{S}}$: (1) Classical: $\mathbb{S}^{Cl} = \mathbb{R}^{F\times F}$  and $ \langle \bm{X}, \bm{Y}\rangle_{\mathbb{S}}^{Cl}= \bm{X}^{T} \bm{Y}$. (2) Global: $\mathbb{S}^{Gl} = c I_F,\; c \in \mathbb{R} $   and $ \langle \bm{X}, \bm{Y}\rangle_{\mathbb{S}}^{Gl}= \text{trace}(\bm{X}^T \bm{Y}) I_F $. (3) Loop-interchange: $\mathbb{S}^{Li}$ is the set of diagonal matrices in $\mathbb{S}$  and $ \langle \bm{X}, \bm{Y}\rangle_{\mathbb{S}}^{Li}= \text{diag}(\bm{X}^T \bm{Y}) $.

In the following we define a block vector subspace  of $\mathbb{R}^{N\times F}$, which will be used later.
\begin{definition}
Given a set of block vectors $\{\bm{X}_k \}_{k=1}^m \subset \mathbb{R}^{N\times F} $, the $\mathbb{S}$-span of $\{\bm{X}_k \}_{k=1}^m$ is defined as $\mathrm{span}^{\mathbb{S}} \{\bm{X}_1, \dots, \bm{X}_m\} \vcentcolon = \{ \sum\limits_{k=1}^{m} \bm{X}_k C_k: C_k \in  \mathbb{S} \}$
\end{definition}
Given the above definition, the order-$m$ block Krylov subspace with respect to $A\in \mathbb{R}^{N\times N},\bm{B}\in \mathbb{R}^{N\times F}$, and  $\mathbb{S}$ can be defined as $\mathcal{K}_m^{\mathbb{S}} (A,\bm{B}) \vcentcolon = \text{span}^{\mathbb{S}} \{\bm{B}, A\bm{B}, \dots, A^{m-1} \bm{B}\} $. The corresponding block Krylov matrix is defined as $K_m (A,\bm{B})\vcentcolon = [ \bm{B}, A\bm{B}, \dots, A^{m-1} \bm{B}]$.

With different definitions of block inner product, we have different  $\mathcal{K}_m^{\mathbb{S}} (A,\bm{B})$ as we will see in Section \ref{conv_in_krylov}.  The one we use in this paper is the classical block inner product and the block Krylov subspace is $\mathcal{K}_m^{Cl} (A,\bm{B}) \vcentcolon= \{ \sum\limits_{k=1}^{m} A^k \bm{B} C_k : C_k \in \mathbb{R}^{F\times F}\}$.
\subsection{Spectral Graph Convolution in Block Krylov Subspace Form}
\label{conv_in_krylov}
In this section, we will show that any graph convolution with well-defined analytic spectral filter defined on $\boldmath{L} \in \mathbb{R}^{N\times N}$ can be written as the product of a block Krylov matrix with a learnable parameter matrix in a specific form.

For any real analytic scalar function $g$, its power series expansion around center 0 can be written as
$$g(x) = \sum\limits_{n=0}^\infty a_n x^n = \sum\limits_{n=0}^\infty \frac{g^{(n) } (0)}{n!} x^n, \; \abs{x} < R$$
where $R$ is the radius of convergence. We can define a filter by $g$.
Let $\rho(L)$ denote the spectrum radius of $L$ and suppose that $\rho(L) <R$. Then the spectral filter $g(L)\in \mathbb{R}^{N \times N}$ can be defined as
$$g(L) \vcentcolon = \sum\limits_{n=0}^\infty a_n L^n =\sum\limits_{n=0}^\infty \frac{g^{(n) } (0)}{n!} L^n, \; \rho(L) < R$$
According to the definition of spectral graph convolution in \eqref{def}, graph signal $\bm{X}$ is filtered by $g(L)$ in the following way,
$$ g(L) \bm{X} = \sum\limits_{n=0}^\infty \frac{g^{(n) } (0)}{n!} L^n \bm{X} =\left[\bm{X}, L\bm{X}, L^2\bm{X}, \cdots  \right] \colvec{\frac{g^{(0) } (0)}{0!} {I_{F}} \\ \frac{g^{(1) } (0)}{1!}{I_{F}}  \\ \frac{g^{(2) } (0)}{2!}{I_{F}}  \\ \vdots  } = A'B'$$
where $A' \in \mathbf{R}^{N \times \infty}$ and $B' \in \mathbf{R}^{\infty \times F}$. It is easy to see that  $A'$ is a block Krylov matrix and Range($A'B'$) $\subseteq$ Range($A'$).  We know that there exists a smallest $m$ such that \cite{frommer2017block}
\begin{equation} \label{krylov}
\text{span}^{\mathbb{S}} \{\bm{X}, L\bm{X}, L^2\bm{X}, \cdots  \} = \text{span}^{\mathbb{S}} \{\bm{X}, L\bm{X}, L^2\bm{X}, \dots,L^{m-1}\bm{X} \},
\end{equation}
\ie{} for any $k\geq m$, $L^k \bm{X} \in \mathcal{K}_m^{\mathbb{S}} (L,\bm{X})$. The value of $m$ depends on $L$ and $\bm{X}$, so we will write it as $m(L,\bm{X})$ later, although here we just use $m$ for simplicity. From \eqref{krylov}, the convolution can be written as
\begin{equation}\label{eq5}
g(L) \bm{X} = \sum\limits_{n=0}^\infty \frac{g^{(n) } (0)}{n!} L^n \bm{X} =\left[\bm{X}, L\bm{X}, L^2\bm{X}, \dots,L^{m-1}\bm{X}  \right] \colvec{ {\Gamma_0}^{\mathbb{S}}  \\ {\Gamma_1}^{\mathbb{S}}   \\ {\Gamma_2}^{\mathbb{S}} \\ \vdots \\ {\Gamma_{m-1}^{\mathbb{S}}} } \equiv  K_m (L,\bm{X}) \Gamma^{\mathbb{S}}
\end{equation}
where ${\Gamma_{i}^{\mathbb{S}}},\; i=1,\dots, m-1$ are parameter matrix blocks. If we use classical inner product, ${\Gamma_{i}^{\mathbb{S}}} \in \mathbb{R}^{F\times F}$. If we use global inner product, $\Gamma_i^{\mathbb{S}} = c_i {I_F}$ and if we use loop-interchange definition, we will have a diagonal $\Gamma_i^{\mathbb{S}}$. When $g(L)\bm{X}$ is multiplied by a parameter matrix $W' \in \mathbb{R}^{F \times O}$,
\begin{equation} \label{eq6}
g(L)\bm{X}W' =  K_m (L,\bm{X}) \Gamma^{\mathbb{S}} W' =  K_m (L,\bm{X}) W^{\mathbb{S}}
\end{equation}
where $W^{\mathbb{S}} \equiv \Gamma^{\mathbb{S}} W' \in \mathbb{R}^{mF \times O}$. The essential  number of learnable parameters is 1) $mF\times O$ for $W^{\mathbb{S}}$ under classical inner product; 2) $FO+m$  for $c_i,W'$ under global inner product; and 3) $mF + FO$ for  $\text{diag}(\Gamma_i^{\mathbb{S}}),W'$ under loop-interchange inner product. We use the classical one for implementation.
\subsection{Deep GCN in Block Krylov Subspace Form}
\label{deep_gcn_krylov}
Since the spectral graph convolution can be simplified as \eqref{eq5}\eqref{eq6}, we can build deep GCN in the following way.
	
Suppose we have a sequence of analytic spectral filters $G=\{ g_0, g_1, \dots, g_n\}$ and a sequence of point-wise nonlinear activation functions $H = \{h_0, h_1,\dots, h_n \}$. Then,
\begin{equation} \label{eq7}
\textbf{Y} = \mbox{softmax} \left\lbrace g_n(L) \; h_{n-1} \left\lbrace  \cdots g_2(L) \; h_1\left\lbrace g_1(L) \; h_0 \left\{ g_0(L) \bm{X} W_0' \right\} W_1' \right\rbrace  W_2' \cdots \right\rbrace W_n' \right\rbrace
\end{equation}
Let us define $\bm{H_0}=\bm{X}$ and $\bm{H_{i+1}}=h_{i} \{ g_i(L) \bm{H_i} W_i'\}, i = 0,\dots,n-1$. Then $\bm{Y}=\mbox{softmax}\{ g_n(L)\bm{H_n} W_n' \}$. From \eqref{eq6}\eqref{eq7}, we have an iterative relation that $\bm{H_{i+1}} = h_{i} \{K_{m_i} (L,\bm{H_i}) W_i^{\mathbb{S}}\},$ where $m_i = m(L,\bm{H_i})$. It is easy to see that, when $g_i(L) = I$, \eqref{eq7} is fully connected network \cite{li2018deeper}; when $g_i(L) = \tilde{A}, \; n=1$, it is just GCN \cite{kipf2016classification}; when $g_i(L)$ is defined by Chebyshev polynomial \cite{hammond2011wavelets}, $W_i' = I$ and under the global inner product, \eqref{eq7} is ChebNet \cite{defferrard2016fast}.

\subsection{Difficulties in Computation}
\label{difficulty}
In the last subsection, we gave a general form of deep GCN in block Krylov form. Following this idea, we can leverage the existing block Arnoldi (Lanczos) algorithm \cite{frommer2017block, frommer2017radau} to compute orthogonal basis of $\mathcal{K}_{m_i}^{\mathbb{S}} (L,\bm{H_i})$ and find $m_i$. But there are some difficulties in practice:
\begin{enumerate}[leftmargin=12pt]
\item During the training phase, $\bm{H_i}$ changes every time that parameters are updated. This makes $m_i$ become a variable and thus requires adaptive size for parameter matrices.
\item The value of $m_i$ can be large when we use global or loop-interchange block inner product. And for classical inner product, the $QR$ factorization that is needed in block Arnoldi algorithm \cite{frommer2017block} is difficult to be put into backpropagation framework.
\end{enumerate}

Although direct implementation of block Krylov methods in GCN is hard, it inspires us that if we have a good way to stack multi-scale information in each hidden layer, the network will have the ability to be extended to deep architectures. We propose a way to alleviate the difficulties in section \ref{deep}
%As Figure 2 shows, diffusion operator of the three popular citation networks are not low-rank; and also for Erd\H{o}s-R\'enyi graph $G(n,p)$ with $p = \omega(\frac{1}{n})$ and $\sigma^2 = p(1-p)$, the empirical spectral distribution of $\frac{1}{\sqrt{n\sigma}} A_n$ converges to Wigner semicircle distribution \cite{tran2013sparse}, which is far from low rank.
%Therefore, we propose a densely connected GCN and truncated block Krylov GCN which can efficiently stack multi-scale information with full $L$ and obtain a deep architecture.
\section{Deep GCN Architectures}
\label{deep}
Upon the theoretical analysis in the last section, we propose two architectures: snowball GCN and truncated block Krylov network. These methods concatenate multi-scale feature information in hidden layers by different ways while both have the potential to adapt to deep architectures.
\subsection{Snowball GCN}
\label{snowball_gcn}
In order to concatenate multi-scale features together and get a richer representation for each node, we design a densely connected graph network (Figure \ref{deep_gcn}(a)) as follows,
\begin{align}
\label{snowball}
&\bm{H_0} = \bm{X},\; \bm{H_{l+1}} = f \left( L \left[ \bm{H_0}, \bm{H_1},\dots, \bm{H_l} \right] W_l \right),\; l=0,1,2,\dots,n - 1 \nonumber \\
& \bm{C} = g \left( \left[ \bm{H_0}, \bm{H_1},\dots, \bm{H_n} \right] W_n \right)\\
&output = \text{softmax} \left( L^{p} \bm{C} W_C \right) \nonumber
\end{align}
where $W_l \in \mathbb{R}^{\left( \sum\limits_{i=0}^l F_i \right)  \times F_{l+1} }, W_n \in \mathbb{R}^{\left( \sum\limits_{i=0}^n F_i \right)  \times F_C}, W_C \in \mathbb{R}^{F_C \times F_O}$ are learnable parameter matrices, $F_{l+1}$ is the number of output channels in layer $l$; $f, g$ are point-wise activation functions; $C$ is a classifier of any kind; $p \in \{0,1\}$. $\bm{H_0}, \bm{H_1},\dots, \bm{H_n}$ are extracted features. $C$ can be a fully connected neural network or even an identity layer with $C=[\bm{H_0}, \bm{H_1},\dots, \bm{H_n}]$. When $p=0$, $L^p=I$ and when $p=1$, $L^P=L$, which means that we project $C$ back onto graph Fourier basis which is necessary when graph structure encodes much information. Following this construction, we can stack all learned features as the input of the subsequent hidden layer, which is an efficient way to concatenate multi-scale information. The size of input will grow like a snowball and this construction is similar to DenseNet \cite{huang2017densely}, which is designed for regular grids (images). Thus, some advantages of DenseNet are naturally inherited, \eg{} alleviate the vanishing-gradient problem, encourage feature reuse, increase the variation of input for each hidden layer, reduce the number of parameters, strengthen feature propagation and improve model compactness.

\subsection{Truncated Block Krylov Network}


As stated in Section \ref{difficulty}, the fact that $m_i$ is a variable makes GCN difficult to be merged into the block Krylov framework. But we can make a compromise and set $m_i$ as a hyperparameter. Then we can get a truncated block Krylov network (Figure \ref{deep_gcn}(b)) as shown below,
\begin{align}
&\bm{H_0} = \bm{X}, \; \bm{H_{l+1}}=f \left( \left[ \bm{H_l}, L \bm{H_l} \dots, L^{m_l -1} \bm{H_l} \right] W_l   \right), \; l=0,1,2,\dots,n - 1 \nonumber \\
& \bm{C} = g \left( \bm{H_n} W_n   \right)\\
&output = \text{softmax} \left(L^{p} \bm{C} W_C \right) \nonumber
\end{align}
where $W_l \in \mathbb{R}^{\left( m_l F_l \right)  \times F_{l+1} }, W_n \in \mathbb{R}^{ F_n \times F_C}, W_C \in \mathbb{R}^{F_C \times F_O}$ are learnable parameter matrices, $f$ and $g$ are activation functions, and $p \in \{0,1\}$.

There is plenty of work on the analysis of error bounds of doing truncation in block Krylov methods \cite{frommer2017block}. But the results need many assumptions either on $\bm{X}$, \eg{} $\bm{X}$ is a standard Gaussian matrix \cite{ wang2015improved}, or on  $L$, \eg{} some conditions on the smallest and largest eigenvalues of $L$  have to be satisfied \cite{musco2018stability}. Instead of doing truncation for a specific function or a fixed $\bm{X}$, we are dealing with variable $\bm{X}$ during training. So we cannot put any restriction on $\bm{X}$ and its relation to $L$ to get a practical error bound.

\begin{figure*}[htbp]
\centering
\subfloat[Snowball]{
\captionsetup{justification = centering}
\includegraphics[width=0.49\textwidth]{{fig_snowball}.pdf}}
\hfill
\subfloat[Truncated Block Krylov]{
\captionsetup{justification = centering}
\includegraphics[width=0.49\textwidth]{{fig_truncated_krylov}.pdf}}
\caption{Deep GCN Architectures}
\label{deep_gcn}
\end{figure*}

Here we would like to mention \cite{liao2019lanczos}, which proposes to do low-rank approximation of $L$ by the Lanczos algorithm. The problem with this technique is that information in $L$ will be lost if $L$ is actually not a low-rank matrix.  If we increase the Lanczos step to keep more information, it will hurt the efficiency of the algorithm. Since most of the graphs we are dealing with have sparse connectivity structures, they are actually not low-rank, \eg{} Erd\H{o}s-R\'enyi graph $G(n,p)$ with $p = \omega(\frac{1}{n})$ \cite{tran2013sparse} and Appendix \rom{4}.
Thus we did not do low-rank approximation in our computations.


\subsection{Equivalence of Linear Snowball GCN and Truncated Block Krylov Network}
\label{linear_snowball}
%Figure \ref{activation_functions} diffusion operator $L$ and ReLU are deadly duo for deep GCN. If we drop $L$, we will get a fully-connected networks which underperform GCN \cite{li2018deeper}. So, we can try to drop ReLU and just use identity function between layers. Under densely connected architecture, the extracted feature will become

Under the snowball GCN architecture, the identity function outperforms ReLU as shown in Figure \ref{activation_functions} and it is easier to train than Tanh. In this part, we will show that a multi-layer linear snowball GCN with identity function as $f$, identity layer as $C$ and $p=1$ is equivalent to a 1-layer block Krylov network with identity layer $C$, $p =1$ and a special parameter matrix.

We write $W_i$ as $W_i=\colvec{\bm{W_i^1}\\ \vdots \\ \bm{W_i^{i+1}}}$ and follow \eqref{snowball} we have
\begin{equation*}
 \bm{H_0} = \bm{X}, \; \bm{H_1} = L\bm{X} W_0, \; \bm{H_2} = L[\bm{X}, \bm{H_1} ]W_1 = L\bm{X} \bm{W_1^1} +  L^2 \bm{X}\bm{ W_0 W_1^2} = L [\bm{X}, L\bm{X}]
 \begin{bmatrix}
 \bm{I} & 0\\
 0 & \bm{W_0}
\end{bmatrix} \colvec{ \bm{W_1^1} \\ \bm{W_1^2} }, \dots %H_3 = L[X, H_1, H_2] W_2
\end{equation*}
As in \eqref{snowball}, we have $L\bm{C}W_C = L[\bm{H_0}, \bm{H_1},\dots, \bm{H_n}] W_C$. Thus we can write
\iffalse
\[\bm{H_3} = L\bm{X} \bm{W_2^1} + L \bm{H_1} \bm{W_2^2} + L \bm{H_2} \bm{W_2^3} =
%LXW_2^1 + L^2 X (W_0 W_2^2 + W_1^2 W_2^3) + L^3 X W_0 W_1^2 W_2^3 =
[\bm{X}, L\bm{X}, L^2 \bm{X}]
\begin{bmatrix}
 \bm{I} & 0 & 0\\
 0 & \bm{I} & 0\\
 0 & 0 & \bm{W_0}
\end{bmatrix}
\begin{bmatrix}
 \bm{I} & 0 & 0\\
 0 & \bm{W_1^2} & 0\\
 0 & 0 & \bm{W_1^1}
\end{bmatrix} \colvec{ \bm{W_2^1} \\ \bm{W_2^2} \\ \bm{W_2^3}}
\]
\fi
\begin{align*}
 & [\bm{H_0}, \bm{H_1} \cdots,  \bm{H_n}] \\
  %\colvec{ W_{n}^1 \\  W_{n}^2\\ \vdots \\  W_{n}^{n} \\ W_{n}^{n+1} }
  =\ &  \ [\bm{X}, L\bm{X}, \cdots,  L^{n} \bm{X}]
%\begin{bmatrix}
%0 & 0 & \cdots & 0 & 0\\[6pt]
% 0 & \bm{I} & \cdots & 0 & 0\\[0pt]
% \vdots & \vdots & \ddots & \vdots & \vdots\\[6pt]
% 0 & 0 & \cdots & \bm{I} & 0\\[6pt]
% 0 & 0 & \cdots & 0 & \bm{I} \\
%\end{bmatrix}
\begin{bmatrix}
 \bm{I} & 0 & \cdots & 0 \\[6pt]
 0 & \bm{I} & \cdots & 0 \\[0pt]
 \vdots & \vdots & \ddots  & \vdots\\[6pt]
 0 & 0 & \cdots  & \bm{W_0} \\
\end{bmatrix}
\begin{bmatrix}
\bm{I} & 0 & \cdots & 0 \\[6pt]
0 & \bm{I} & \cdots & 0 \\[0pt]
 \vdots & \vdots & \ddots  & \vdots\\[6pt]
 0 & 0 & \cdots &  \bm{{W_1^1}} \\
\end{bmatrix}
\cdots
\begin{bmatrix}
 \bm{I} & 0 & \cdots & 0\\[6pt]
 0 & \bm{W_{n-1}^{n}} & \cdots & 0 \\[0pt]
 \vdots & \vdots & \ddots & \vdots\\[6pt]
 0 & 0 & \cdots &  \bm{W_{n-1}^1} \\
\end{bmatrix}
 %\colvec{ W_{n}^1 \\  W_{n}^2\\ \vdots \\  W_{n}^{n} \\ W_{n}^{n+1} }
\end{align*}
which is in the form of \eqref{eq6}, where the parameter matrix is the multiplication of a sequence of block diagonal matrices whose entries consist of identity blocks and blocks from other parameter matrices.

\section{Experiments}\label{sec:experiments}
We test truncated block Krylov network ($f$=Tanh, $g$=Tanh, $p = 0$), snowball GCN ($f$=Tanh, $g$=identity, $p=1$), linear snowball GCN ($f$=identity, $g$=identity $p=1$) and linear snowball GCN with Tanh ($f$=identity, $g$=Tanh, $p=1$) on public splits \cite{yang2016revisiting,liao2019lanczos} of Cora, Citeseer and Pubmed\footnote{Source code to be found at \url{https://github.com/PwnerHarry/Stronger_GCN}}. In addition, we manually shrink the training sets as \cite{liao2019lanczos,li2018deeper,sun2019stage} to increase the difficulty of the tasks. We compare with the results of several modern algorithms which allow validation, including graph convolutional networks for fingerprint (GCN-FP) \cite{duvenaud2015convolutional}, gated graph neural networks (GGNN) \cite{li2015gated}, diffusion convolutional neural networks (DCNN) \cite{atwood2015diffusion}, Chebyshev networks (ChebNet) \cite{defferrard2016fast}, graph convolutional networks (GCN) \cite{kipf2016classification}, message passing neural networks (MPNN) \cite{gilmer2017neural}, graph sample and aggregate (GraphSAGE) \cite{hamilton2017inductive}, graph partition neural networks (GPNN) \cite{liao2018graph}, graph attention networks (GAT) \cite{velivckovic2017attention}, LanczosNet (LNet) \cite{liao2019lanczos} and AdaLanczosNet (AdaLNet) \cite{liao2019lanczos}. We also compare with some algorithms that do no allow validation, including label propagation using ParWalks (LP) \cite{wu2012learning}, Co-training \cite{li2018deeper}, Self-training \cite{li2018deeper}, Union \cite{li2018deeper}, Intersection \cite{li2018deeper}, GCN without validation \cite{li2018deeper}, Multi-stage training \cite{sun2019stage}, Multi-stage self-supervised (M3S) training \cite{sun2019stage}, GCN with sparse virtual adversarial training (GCNSVAT) \cite{sun2019virtual} and GCN with dense virtual adversarial training (GCNDVAT) \cite{sun2019virtual}.

\begin{figure*}[htbp]
\centering
\subfloat[Linear Snowball]{
\captionsetup{justification = centering}
\includegraphics[width=0.24\textwidth]{{fig_tsne_linear_snowball_cora}.pdf}}
\hfill
\subfloat[Linear Tanh Snowball]{
\captionsetup{justification = centering}
\includegraphics[width=0.24\textwidth]{{fig_tsne_linear_tanh_snowball_cora}.pdf}}
\hfill
\subfloat[Snowball]{
\captionsetup{justification = centering}
\includegraphics[width=0.24\textwidth]{{fig_tsne_snowball_cora}.pdf}}
\hfill
\subfloat[Truncated Krylov]{
\captionsetup{justification = centering}
\includegraphics[width=0.24\textwidth]{{fig_tsne_truncated_krylov_cora}.pdf}}
\caption{t-SNE for the extracted features of models trained on Cora (7 classes) public (5.2\%) using the best hyperparameter settings.}
\label{tsne}
\end{figure*}

\par
For each test case, we use the best hyperparameters to run $10$ independent times to get the average precision. The hyperparameters include learning rate and weight decay for the optimizer RMSprop, taking values in the intervals $[{10}^{-6}, 5\times{10}^{-3}]$ and $[{10}^{-5}, 5\times{10}^{-2}]$, respectively, width of hidden layers taking value in the set $\{16, 32, 64, 128, 256, 512, 1024, 2048, 4096\}$, number of hidden layers in the set $\{1, 2, \dots, 15\}$, dropout in $(0, 0.99]$, and the number of Krylov blocks taking value in $\{4, 5, \dots, 25\}$. The hyperparameter values of the test cases will be presented in the appendix.

\par
To get achieve good training, we use adaptive number of episodes (but no more than $3000$): the early stopping counter is set to be 100.

\input{tab_results_no_validation.tex}
\input{tab_results_with_validation.tex}
We see that the proposed arhitectures achieve overwhelming performance in almost all cases. It is particularly worth noting that when the training sets are small, the proposed architectures do surprisingly better than the existing methods. From the t-SNE\cite{maaten2008visualizing} visualization of the output layers in Figure \ref{tsne}, we see that the architectures can extract good features with small training data. Especially for truncated block Krylov network, it can seperate different classes pretty well. What also impresses us is linear Snowball GCN, which can achieves state-of-the-art performance with much less computational costs or training time.

\section{Future Works}
There are still some future works we need to do: 1) We need to further investigate how the point-wise nonlinear activation functions influence block vectors (\eg{} the feature block vector $\bm{X}$ and hidden feature block vectors $H_i$) so that we can find better activation functions than Tanh; 2) We will try to find a better way to leverage the block Krylov algorithms instead of simply doing truncation.

\clearpage
\bibliographystyle{abbrv}
\bibliography{references}

\newpage

\section*{Appendices}
\label{appendix}
\subsection*{Appendix \rom{1}: Proof of Theorem 1, 2} \label{appendix:1}
We extend Theorem 1 in \cite{li2018deeper} to a general diffusion operator $L$ in the following lemma.
\begin{lemma} 1
Suppose that a graph $\mathcal{G}$ has $k$ connected components $\{C_i\}_{i=1}^k$ and the diffusion operator $L$ is defined as that in \eqref{eq0}. $L$ has $k$ linearly independent eigenvectors $\{\bm{v_1},\dots, \bm{v_k}\}$ corresponding to its largest eigenvalue $\lambda_{max}$. If $\mathcal{G}$ has no bipartite components, then for any $\bm{x} \in \mathbb{R}^N$
\begin{equation}\label{eq2}
\underset{m \rightarrow \infty }{\text{lim}}\; (\frac{1}{\lambda_{max}} L )^m \bm{x} = [\bm{v_1},\dots, \bm{v_k}] \bm{\theta},
\end{equation}
for some $\bm{\theta} \in \mathbb{R}^k$.
\end{lemma}

%% {(\bf What does "uniformly sample" mean? A uniform distribution is for a finite region. Note that you cannot pick a real number uniformly in the 1-dimensional case), I agree, but please check: \url{https://math.stackexchange.com/questions/85955/is-there-a-uniform-distribution-over-the-real-line} and \url{https://en.wikipedia.org/wiki/Prior_probability#Improper_priors}}, then
%\begin{equation}\label{eq:two_vec}
%\doubleP\left(\text{rank}\left(\text{ReLU}([\bm{x},\bm{y}])\right) \leq %\text{rank}([\bm{x},\bm{y}]) \;|\;\bm{x},\bm{y} \in \mathbb{R}^N\right) = 1
%\end{equation}
%\end{lemma}
%\begin{proof}
%See Appendix \rom{1}.
%\end{proof}

\begin{lemma} 2
Suppose we  randomly sample $\bm{x}, \bm{y}\in \mathbb{R}^N$ under a continuous distribution. Suppose we have point-wise function $\text{ReLU}(z) = \text{max}(0,z)$, we have
$$\doubleP(\text{rank}\left(\text{ReLU}([\bm{x}, \bm{y}])\right) \leq \text{rank}([\bm{x}, \bm{y}]) \;|\; \bm{x}, \bm{y} \in \mathbb{R}^N) = 1$$
\end{lemma}
\begin{proof}
We generalize ReLU onto multi-dimensional case by applying it element-wise on every element of the matrix. Any $\bm{x} \in \mathbb{R}^N$ can be represented as $\bm{x} = \bm{x_+} + \bm{x_-}$, where $\bm{x_+}$ and $\bm{x_-}$ are the nonnegative and nonpositive components of $\bm{x}$, respectively. We can see that $\text{ReLU}(\bm{x}) = \bm{x}_+$. It is trivial when $\bm{y}=0$. We only discuss for all nonzero $\bm{y} \in \mathbb{R}^N$.

Suppose $\bm{x}$ and $\bm{y}$ are linearly independent (with probability $1$), then $\nexists\; c \neq 0$ that $\bm{x} = c\bm{y}$. If $\exists\; d \neq 0$ that ReLU$(\bm{x})$= $d$ ReLU$(\bm{y})$, then $\bm{x_+} = d \bm{y_+} $ and $\bm{x_-} \neq d \bm{y_-}$ and the existence of this kind of $\bm{x},\bm{y}$ has a nonzero probability $\frac{1}{2^N} \frac{1+N}{2^N}$ (See Lemma 3); other than these cases, the independency will be kept after the ReLU transformation. %This means that point-wise ReLU weakens the linear independency between vectors with probability 1.

Suppose $y$ is linearly dependent of $\bm{x}$ (with probability 0), \ie{}, $\exists\; c \neq 0$ that $\bm{x} = c \bm{y}$. If $c>0$, we have $\bm{x_-} = c \bm{y_-}$ and $ \bm{x_+} = c \bm{y_+}$. Since ReLU$(\bm{x})$=$\bm{x_+}$, ReLU$(\bm{y})$=$\bm{y_+}$, then ReLU$(\bm{x})$= $c$ReLU$(\bm{y})$. If $c<0$, we have $\bm{x_-} = c \bm{y_+}$ and $ \bm{x_+} = c \bm{y_-}$. If $\exists\; d \neq 0$ that ReLU$(\bm{x})$= $d$ ReLU$(\bm{y})$, this means $\bm{x_+} = d \bm{y_+} = \frac{d}{c} \bm{x_-}$. This $d$ exsits when $\bm{x_+} $ and $\bm{x_-}$ are linearly dependent, which only holds when $x=0$. This happens with probability 0. Then, ReLU$(\bm{x})$ and $d$ ReLU$(\bm{y})$ will be independent. So whether ReLU keep the dependency between to vectors or not depends on the sign of $c$. Thus, under the assumption, we have probability $\frac{1}{2}$ that ReLU keeps the dependency.

According to the discussion above, we have
%Unless $\bm{x_-} \neq 0$ and $\bm{x_+} \neq 0$, the linear dependency still holds. The overall probability of losing dependency between column vectors is 0. then this case is trivial.% since it happens with probability 0.
\begin{align*}
&\doubleP\left(\text{rank}\left(\text{ReLU}([\bm{x}, \bm{y}])\right) \leq \text{rank}([\bm{x}, \bm{y}]) \;|\;\bm{x}, \bm{y} \in \mathbb{R}^N\right) = \frac{\doubleP\left(\text{rank}\left(\text{ReLU}([\bm{x}, \bm{y}])\right) \leq \text{rank}([\bm{x}, \bm{y}]) ,\bm{x}, \bm{y} \in \mathbb{R}^N\right)}{\doubleP\left(\bm{x}, \bm{y} \in \mathbb{R}^N\right)}\\
& = \doubleP\left(\text{rank}\left(\text{ReLU}([\bm{x}, \bm{y}])\right) \leq \text{rank}([\bm{x}, \bm{y}]) \;|\; \text{rank}([\bm{x}, \bm{y}]) = 1,\bm{x}, \bm{y} \in \mathbb{R}^N \right) \doubleP\left(\text{rank}([\bm{x}, \bm{y}]) = 1,\bm{x}, \bm{y} \in \mathbb{R}^N \right) +\\
&\doubleP\left(\text{rank}\left(\text{ReLU}([\bm{x}, \bm{y}])\right) \leq \text{rank}([\bm{x}, \bm{y}]) \;|\; \text{rank}([\bm{x}, \bm{y}]) = 2,\bm{x}, \bm{y} \in \mathbb{R}^N \right) \doubleP\left(\text{rank}([\bm{x}, \bm{y}]) = 2,\bm{x}, \bm{y} \in \mathbb{R}^N \right)\\
& = 0 \times \frac{1}{2} + 1 \times 1 = 1.
\end{align*}
Lemma proved.
\end{proof}
\begin{lemma} 3
 Suppose we randomly sample $\bm{x}, \bm{y} \in \mathbb{R}^N$ under a continuous distribution, then
 $$\doubleP(\bm{x_+} = d \bm{y_+}, \bm{x_-} \neq d \bm{y_-} \;|\; x,y \in \mathbb{R}^N, d \neq 0) = \frac{1}{2^N} \frac{1+N}{2^N}$$
\end{lemma}
\begin{proof}
\begin{align*}
    &\doubleP(\bm{x_+} = d \bm{y_+}, \bm{x_-} \neq d \bm{y_-}) \\
    & = P (\bm{x_+} = d \bm{y_+}, \bm{x_-} \neq d \bm{y_-} \;|\;  df(\bm{x_+}) \leq 1 ) \; P(df(\bm{x_+}) \leq 1) + P (\bm{x_+} = d \bm{y_+}, \bm{x_-} \neq d \bm{y_-} \;|\;  df(\bm{x_+}) > 1 ) \; P (df(\bm{x_+}) > 1)\\
    &=\frac{1}{2^N} \frac{1+N}{2^N} + 0 \cdot \frac{2^N -1 -N}{2^N} =\frac{1}{2^N} \frac{1+N}{2^N}
\end{align*}
where $df$ denotes degree of freedom. $df(\bm{x_+}) \leq 1$ means that $\bm{x}$ can at most have one dimension to be positive and there are $1+N$ out of $2^N$ hyperoctants that satisfies this condition. The set of $\bm{y}$ that can make $\bm{x_+} = d \bm{y_+}, \bm{x_-} \neq d \bm{y_-}$ hold has an area of $\frac{1}{2^N}$, \ie{} when $\bm{y}$ is in the same hyperoctant as $\bm{x}$. If $\bm{x}$ lies in other hyperoctants, $df(y_-) \leq N-2$. And since $\bm{x_+} = d \bm{y_+}$, $\bm{y}$ is just a low dimensional surface in $\mathbb{R}^N$ with area 0.
\end{proof}
\begin{theorem} 1
Suppose that $\mathcal{G}$ has $k$ connected components and the diffusion operator $L$ is defined as that in \eqref{eq0}. Let $\bm{X} \mathbb{R}^{N \times F}$ be any block vector that randomly sampled under a continuous distribution and $\{W_0, W_1, \dots, W_n\}$ be any set of parameter matrices, if $\mathcal{G}$ has no bipartite components, then in \eqref{eq1}, as $n \to \infty$, $\text{rank}(\bm{Y'}) \leq k$ almost surely.
%, consider the space $\mathbb{R}^{N \times F}$ of all block vectors $\bm{X}$ and any set of parameter matrices $\{W_0, W_1, \dots, W_n\}$, we have $\text{rank}(\bm{Y'}) \leq k$ with probability 1, as $n\rightarrow \infty$.
\end{theorem}	

\begin{proof}
Upon the conclusions in Lemma 2-3, we have rank$\left(\text{ReLU} (L \textbf{X})\right) \leq$rank($L \textbf{X}$) with probability 1 and it is obvious rank$(L \bm{X} W_0) \leq$rank($L \bm{X}$). Using these two inequality iteratively for \eqref{eq1}, we have rank($\bm{Y'}$) $\leq$ rank($L^{n+1}  \bm{X}$). Based on Lemma 1, we have probability 1 to get
\begin{equation*} \label{eq3}
\underset{n \rightarrow \infty }{\text{lim}} \; \text{rank}(\bm{Y'}) \leq \underset{n \rightarrow \infty }{\text{lim}}\; \text{rank}(L^{n+1} \bm{X}) = \text{rank} ([\bm{v_1},\dots, \bm{v_k}] [\bm{\theta_1},\dots, \bm{\theta_F}]) \leq \text{rank}([\bm{v_1},\dots, \bm{v_k}]) = k,
\end{equation*}
where $\bm{\theta_i} \in \mathbb{R}^k, i=1,\dots,F$. Thus, rank($\bm{Y'}$) $\leq k$
\end{proof}

%\begin{lemma} 4
 %Given $\bm{x}, y\in \mathbb{R}^2$ and point-wise function $\text{Tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$, we have
%$$\doubleP(\text{rank}\left(\text{Tanh}([x,y])\right) \geq \text{rank}([x,y]) \;|\; x,y \in \mathbb{R}^N) = 1 $$
%\end{lemma}


\begin{theorem} 2
Suppose we randomly sample $\bm{x}, \bm{y} \in \mathbb{R}^N$ under a continuous distribution and we have the point-wise function $\text{Tanh}(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$, we have
$$\doubleP(\text{rank}\left(\text{Tanh}([\bm{x},\bm{y}])\right) \geq \text{rank}([\bm{x},\bm{y}]) \;|\; \bm{x},\bm{y} \in \mathbb{R}^N) = 1$$
\end{theorem}

\begin{proof}
We first prove it for $N=2$. It is trivial when $x,y$ have 0 elements.

Suppose $\bm{x} = [x_1, x_2] ,\bm{y} = [y_1, y_2]$ are linearly dependent and all of their elements are nonzero. If $\text{Tanh}(\bm{x})$ and $\text{Tanh}(\bm{y})$ are still linearly dependent, we must have
$$ \frac{\text{Tanh}(x_1)}{\text{Tanh}(y_1)} = \frac{\text{Tanh}(x_2)}{\text{Tanh}(y_2)}, \text{ \st{} } \frac{x_1}{y_1} = \frac{x_2}{y_2} $$
This equation only have one solution
$$x_1 = \frac{1033977}{9530},\; x_2 = -\frac{929}{10} ,\; y_1 = -\frac{1113}{10}, \; y_2 = \frac{953}{10}$$
which means point-wise $\text{Tanh}$ transformation will break the dependency of $\bm{x},\bm{y}$ with probability 1. %their linear dependency will be kept only when $x=y$, which is trivial.

If $x,y$ are linearly independent, suppose $\text{Tanh}(\bm{x}) = \bm{x'} = [x_1', x_2']$ is a vector in $\mathbb{R}^2$, and the set of vectors in $\mathbb{R}^2$ that can be transformed by point-wise Tanh to the same line as $\bm{x'}$ covers an area of probability 0. That is for any fixed $\bm{x'} \in \mathbb{R}^2$, the solution of
$$\frac{\text{Tanh}(y_1)}{\text{Tanh}(y_2)} = \frac{x_1'}{x_2'}$$
covers an area of 0. Thus,
$$ \doubleP(\text{rank}\left(\text{Tanh}([\bm{x},\bm{y}])\right) \geq \text{rank}([\bm{x},\bm{y}]) \;|\; \bm{x},\bm{y} \in \mathbb{R}^N) = 1 $$
which means point-wise Tanh transformation will increase the independency between vectors in $\mathbb{R}^2$.

Similar to $\mathbb{R}^2$, suppose $\bm{x} = [x_1, x_2, \dots, x_N] ,\bm{y} = [y_1, y_2, \dots, y_N]$ are linearly dependent, if all elements in $\bm{x}, \bm{y}$ are nonzero and $\text{Tanh}(\bm{x})$ and $\text{Tanh}(\bm{y})$ are still linearly dependent, we must have
$$ \frac{\text{Tanh}(x_1)}{\text{Tanh}(y_1)} = \frac{\text{Tanh}(x_2)}{\text{Tanh}(y_2)} = \cdots = \frac{\text{Tanh}(x_N)}{\text{Tanh}(y_N)}, \text{ \st{} } \frac{x_1}{y_1} = \frac{x_2}{y_2} \cdots = \frac{x_N}{y_N} $$
The solution of any pair of equations covers an area of probability 0 in $\mathbb{R}^N$. Actually, this still holds when $\bm{x},\bm{y}$ have some 0 elements, \ie{} for any subset of the above equations, the area of the solution is still 0.

If $\bm{x},\bm{y}$ are linearly independent, suppose $\text{Tanh}(\bm{x}) = \bm{x'}$ is a vector in $\mathbb{R}^N$, and the space in $\mathbb{R}^N$ that can be transformed by point-wise $\text{Tanh}$ to the same line as $\bm{x'}$ covers an area of probability 0. Therefore, Lemma 2 still holds in $\mathbb{R}^N$.
\end{proof}

\subsection*{Appendix \rom{2}: Numerical Experiments on Synthetic Data}
\label{appendix:2}
The goal of the experiments is to test which network structure with which kind of activation function has the potential to be extended to deep architecture. We measure this potential by the numerical rank of the output features in each hidden layer of the networks using synthetic data. The reason of choosing this measure can be explained by Theorem \ref{thm1}. We build the certain networks with depth 100 and the data is generated as follows.

We first randomly generate edges of an Erd\H{o}s-R\'enyi graph $G(1000,0.01)$, \ie{} the existance of the edge between any pair of nodes is a Bernoulli random variable with $p = 0.01$.  Then, we construct the corresponding adjacency matrix $A$ of the graph which is a $\mathbb{R}^{1000 \times 1000} $ matrix. We generate a $\mathbb{R}^{1000 \times 500}$ feature matrix $\bm{X}$ and each of its element is drawn from $N(0,1)$. We normalize $A$ and $\bm{X}$ as \cite{kipf2016classification} and abuse the notation $A,\bm{X}$ to denote the normalized matrices. We keep 3 blocks in each layer of truncated block Krylov network. The number of input channel in each layer depends on the network structures and the number of output channel is set to be 128 for all networks. Each element in every parameter matrix $W_i, \; i = 1, \dots, 100$ is randomly sampled from $N(0,1)$ and the size is $\mathbb{R}^{\# input \times \#output}$. With the synthetic $A,\bm{X},W_i$, we simulate the feedforward process according to the network architecture and collect the numerical rank (at most 128) of the output in each of the 100 hidden layers. For each activation function under each network architecture, we repeat the experiments for 20 times and plot the mean results with standard deviation bars.
\subsection*{Appendix \rom{3}: Rank Comparison of Activation Functions and Networks}
\label{appendix:3}
\begin{figure*}[htbp]
\centering
\subfloat[GCN]{
\captionsetup{justification = centering}
\includegraphics[width=0.32\textwidth]{{fig_GCN_compare_all}.pdf}}
\hfill
\subfloat[Snowball]{
\captionsetup{justification = centering}
\includegraphics[width=0.32\textwidth]{{fig_snowball_compare_all}.pdf}}
\hfill
\subfloat[Truncated Block Krylov]{
\captionsetup{justification = centering}
\includegraphics[width=0.32\textwidth]{{fig_truncated_krylov_compare_all}.pdf}}
\caption{Column ranks of different activation functions with the same architecture}
\end{figure*}

\begin{figure*}[htbp]
\centering
\subfloat[ReLU]{
\captionsetup{justification = centering}
\includegraphics[width=0.32\textwidth]{{fig_relu_compare}.pdf}}
\hfill
\subfloat[Identity]{
\captionsetup{justification = centering}
\includegraphics[width=0.32\textwidth]{{fig_identity_compare}.pdf}}
\hfill
\subfloat[Tanh]{
\captionsetup{justification = centering}
\includegraphics[width=0.32\textwidth]{{fig_tanh_compare}.pdf}}
\caption{Column ranks of different architectures with the same activation function}
\end{figure*}

\subsection*{Appendix \rom{4}: Spectrum of the Datasets}
\label{appendix:4}
%See figure \ref{spectrum_dataset}.
\label{spectrum}
\begin{figure*}[bhtp]
\centering
\subfloat[Cora]{
\captionsetup{justification = centering}
\includegraphics[width=0.32\textwidth]{{fig_spectrum_cora}.pdf}}
\hfill
\subfloat[Citeseer]{
\captionsetup{justification = centering}
\includegraphics[width=0.32\textwidth]{{fig_spectrum_citeseer}.pdf}}
\hfill
\subfloat[PubMed]{
\captionsetup{justification = centering}
\includegraphics[width=0.32\textwidth]{{fig_spectrum_pubmed}.pdf}}
\caption{Spectrum of the renormalized adjacency matrices for several datasets}
\label{spectrum_dataset}
\end{figure*}

%\subsection{Appendix \rom{7}: Adaptive Depth}
%In practice, it is found that the performance of the training model is sensitive to the position of the training data in graph. If many of the training nodes happen to be located at some pivotal positions, \eg{} the center of a densely connected cluster, it is much faster to diffuse the feature signals across the graph. Otherwise, if a large part of our training nodes are isolated nodes or have few links with others, it will take much more steps for the signal to diffuse to other parts of the graph. Therefore, before we train the model, we should take an extra step to decide how many diffusion steps we need, \ie{} the number of layers of our network. If the depth is too shallow, the signal cannot diffuse to some unlabeled nodes. On the other hand, if the network is too deep, we have overfitting problem and our prediction are likely to be interfered by some noise signals from some pretty far nodes. So we try to design a method that the training data can deliver enough message to reachable nodes but the reachable nodes do not receive too much redundant message.
%Another way is we compare how many new nodes that can be reach and how many new edges added to the reached nodes.
%By using adaptive layer numbers, the final training results become more stable but there still exists some cases that you cannot every get a good upper bound. For this reason, we try to sacrifice the efficiency and directly use a large Dense GCN for any case.

\subsection*{Appendix \rom{5}: Experiment Settings and Hyperparameters}
\label{appendix:5}
The so-called public splits in \cite{liao2019lanczos} and the setting that randomly sample 20 instances for each class as labeled data in \cite{yang2016revisiting} is actually the same. Most of the results for the algorithms with validation are cited from \cite{liao2019lanczos}, where they are reproduced with validation. However, some of them actually do not use validation in original papers and can achieve better results. In the paper, We compare with their best results.

We use NVIDA apex amp mixed-precision plugin for PyTorch to accelerate our experiments. Most of the results were obtained from NVIDIA V100 clusters on Beluga of Compute-Canada, with minor part of them obtained from NIVIDIA K20, K80 clusters on Helios Compute-Canada. The hyperparameters are searched using Bayesian optimization.

A useful tip is the smaller your training set is, the larger dropout probability should be set and the larger early stopping you should have.

Table \ref{tab:hyperparameters_without_validation} and Table \ref{tab:hyperparameters_with_validation} shows the hyperparameters to achieve the performance in the experiments, for cases without and with validation, respectively. When conducting the hyperparameter search, we encounter memory problems: current GPUs cannot afford deeper and wider structures. But we do observe better performance with the increment of the network size. It is expected to achieve better performance with more advanced deep learning devices.

\input{tab_hyperparameters_with_validation.tex}

\input{tab_hyperparameters_without_validation.tex}

\end{document}
